"""
è·å– Dancing With The Stars æ˜æ˜Ÿçš„ Google Trends æ•°æ®
ä½¿ç”¨é”šç‚¹å½’ä¸€åŒ–æ–¹æ³•ï¼Œä½¿ä¸åŒæ˜æ˜Ÿä¹‹é—´çš„æ•°æ®å¯ä»¥æ¯”è¾ƒ
åŒ…å«å¼ºå¤§çš„åå°é”æœºåˆ¶å’Œæ–­ç‚¹ç»­ä¼ åŠŸèƒ½
"""

import pandas as pd
from pytrends.request import TrendReq
from pytrends.exceptions import ResponseError
import os
import time
import random
import json
import sys
from datetime import datetime
import urllib3

# 1. ç¦ç”¨ SSL è­¦å‘Šï¼ˆä¸ºäº†åé¢ verify=False ä¸æŠ¥é”™ï¼‰
urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

# 2. å¼ºåˆ¶è®¾ç½®ç³»ç»Ÿçº§ä»£ç†
# è¯·ç¡®ä¿è¿™é‡Œçš„ç«¯å£å· (7890) å’Œä½  VPN è½¯ä»¶é‡Œçš„ä¸€è‡´ï¼
# å¦‚æœæ˜¯ v2rayNï¼Œå¯èƒ½æ˜¯ 10809
PROXY_PORT = '7890' 
os.environ['HTTP_PROXY'] = f'http://127.0.0.1:{PROXY_PORT}'
os.environ['HTTPS_PROXY'] = f'http://127.0.0.1:{PROXY_PORT}'

# ============== é…ç½®å‚æ•° ==============
DATA_DIR = os.path.dirname(os.path.abspath(__file__))
SEASON_TIME_FILE = os.path.join(DATA_DIR, 'season-time.csv')
CELEBRITY_DATA_FILE = os.path.join(DATA_DIR, '2026_MCM_Problem_C_Data.csv')
OUTPUT_DIR = os.path.join(DATA_DIR, 'get_data')

# é”šç‚¹å…³é”®è¯ - ç”¨äºå½’ä¸€åŒ–æ•°æ®
ANCHOR_KEYWORD = "Dancing with the Stars"

# æ‰¹é‡çˆ¬å–è®¾ç½®
BATCH_SIZE = 4  # æ¯æ‰¹æœ€å¤š4ä¸ªæ˜æ˜Ÿï¼ˆåŠ ä¸Šé”šç‚¹å…±5ä¸ªï¼Œæ˜¯Google Trendsçš„ä¸Šé™ï¼‰

# è¯·æ±‚å»¶è¿Ÿè®¾ç½®ï¼ˆç§’ï¼‰
MIN_DELAY = 10  # æœ€å°å»¶è¿Ÿ
MAX_DELAY = 20  # æœ€å¤§å»¶è¿Ÿ

# å’–å•¡ä¼‘æ¯è®¾ç½®
COFFEE_BREAK_INTERVAL = 10  # æ¯å¤„ç†10ä¸ªæ˜æ˜Ÿä¼‘æ¯ä¸€æ¬¡
COFFEE_BREAK_DURATION = 60  # å’–å•¡ä¼‘æ¯æ—¶é•¿ï¼ˆç§’ï¼‰

# é‡è¯•è®¾ç½®
MAX_RETRIES = 5  # æœ€å¤§é‡è¯•æ¬¡æ•°
RATE_LIMIT_SLEEP = 300  # é‡åˆ°429é”™è¯¯æ—¶çš„ç­‰å¾…æ—¶é—´ï¼ˆ5åˆ†é’Ÿï¼‰
NORMAL_RETRY_DELAY = 30  # æ™®é€šé”™è¯¯çš„é‡è¯•å»¶è¿Ÿ


def countdown_sleep(seconds, message="ä¼‘æ¯ä¸­"):
    """å¸¦å€’è®¡æ—¶æ˜¾ç¤ºçš„ä¼‘çœ å‡½æ•°"""
    for remaining in range(int(seconds), 0, -1):
        sys.stdout.write(f"\r  {message}... å‰©ä½™ {remaining} ç§’   ")
        sys.stdout.flush()
        time.sleep(1)
    sys.stdout.write("\r" + " " * 50 + "\r")  # æ¸…é™¤å€’è®¡æ—¶è¡Œ
    sys.stdout.flush()


def load_season_times():
    """ä»CSVåŠ è½½èµ›å­£æ’­å‡ºæ—¶é—´æ®µ"""
    df = pd.read_csv(SEASON_TIME_FILE)
    season_times = {}
    for _, row in df.iterrows():
        season = int(row['Season'])
        start_date = row['Start_Date']
        end_date = row['End_Date']
        season_times[season] = (start_date, end_date)
    print(f"âœ“ å·²åŠ è½½ {len(season_times)} ä¸ªèµ›å­£çš„æ—¶é—´æ•°æ®")
    return season_times


def load_celebrities():
    """ä»CSVåŠ è½½æ˜æ˜Ÿæ•°æ®"""
    df = pd.read_csv(CELEBRITY_DATA_FILE)
    celebrities = df[['celebrity_name', 'season']].copy()
    celebrities['season'] = celebrities['season'].astype(int)
    print(f"âœ“ å·²åŠ è½½ {len(celebrities)} æ¡æ˜æ˜Ÿè®°å½•")
    return celebrities


def create_fresh_pytrends():
    """
    åˆ›å»ºæ–°çš„pytrendsä¼šè¯ï¼ˆæ¯æ¬¡è¯·æ±‚é‡æ–°åˆå§‹åŒ–ä»¥é¿å…å°é”ï¼‰
    
    æ³¨æ„ï¼šä¸ä½¿ç”¨ retries å’Œ backoff_factor å‚æ•°ï¼Œå› ä¸ºåœ¨æ–°ç‰ˆæœ¬ urllib3 ä¸­
    è¿™äº›å‚æ•°å¯èƒ½å¯¼è‡´ 'method_whitelist' å…¼å®¹æ€§é—®é¢˜ã€‚
    æˆ‘ä»¬åœ¨å¤–å±‚ä»£ç ä¸­æ‰‹åŠ¨å¤„ç†é‡è¯•é€»è¾‘ã€‚
    """

    return TrendReq(
        hl='en-US', 
        tz=360,
        timeout=(10, 25),  # è¿æ¥è¶…æ—¶10ç§’ï¼Œè¯»å–è¶…æ—¶25ç§’
        retries=2,  # ç®€å•é‡è¯•æ¬¡æ•°
    )


def get_google_trends_with_anchor(celebrity_names, start_date, end_date, geo='US'):
    """
    ä½¿ç”¨é”šç‚¹æ–¹æ³•æ‰¹é‡è·å–Google Trendsæ•°æ®
    
    Args:
        celebrity_names: æ˜æ˜Ÿå§“ååˆ—è¡¨ï¼ˆæœ€å¤š4ä¸ªï¼‰
        start_date: å¼€å§‹æ—¥æœŸ 'YYYY-MM-DD'
        end_date: ç»“æŸæ—¥æœŸ 'YYYY-MM-DD'
        geo: åœ°åŒºï¼ˆé»˜è®¤ï¼šç¾å›½ï¼‰
    
    Returns:
        åŒ…å«å½’ä¸€åŒ–æ•°æ®çš„DataFrameï¼Œå¤±è´¥è¿”å›None
        è¿”å›çš„DataFrameåŒ…å«æ‰€æœ‰æ˜æ˜Ÿçš„æ•°æ®å’Œå„è‡ªçš„å½’ä¸€åŒ–åˆ†æ•°
    """
    if isinstance(celebrity_names, str):
        celebrity_names = [celebrity_names]
    
    if len(celebrity_names) > BATCH_SIZE:
        print(f"  âš  è­¦å‘Š: ä¸€æ¬¡æœ€å¤šæŸ¥è¯¢{BATCH_SIZE}ä¸ªæ˜æ˜Ÿï¼Œå½“å‰{len(celebrity_names)}ä¸ª")
        celebrity_names = celebrity_names[:BATCH_SIZE]
    
    timeframe = f'{start_date} {end_date}'
    # æ˜æ˜Ÿåˆ—è¡¨ + é”šç‚¹å…³é”®è¯ï¼ˆæœ€å¤š5ä¸ªï¼‰
    kw_list = celebrity_names + [ANCHOR_KEYWORD]
    
    last_error = None
    
    for attempt in range(MAX_RETRIES):
        try:
            # æ¯æ¬¡è¯·æ±‚åˆ›å»ºæ–°çš„session
            pytrends = create_fresh_pytrends()
            
            pytrends.build_payload(
                kw_list,
                cat=0,
                timeframe=timeframe,
                geo=geo,
                gprop=''
            )
            
            data = pytrends.interest_over_time()
            
            if data.empty:
                print(f"  âš  æœªæ‰¾åˆ°æ•°æ®")
                return None
            
            # ç§»é™¤ 'isPartial' åˆ—
            if 'isPartial' in data.columns:
                data = data.drop(columns=['isPartial'])
            
            # ä¸ºæ¯ä¸ªæ˜æ˜Ÿè®¡ç®—å½’ä¸€åŒ–åˆ†æ•°
            for celeb_name in celebrity_names:
                if celeb_name in data.columns:
                    norm_col = f'{celeb_name}_normalized'
                    data[norm_col] = (data[celeb_name] / data[ANCHOR_KEYWORD]) * 100
                    # å¤„ç†é™¤é›¶äº§ç”Ÿçš„æ— ç©·å€¼å’ŒNaN
                    data[norm_col] = data[norm_col].replace([float('inf'), float('-inf')], 0)
                    data[norm_col] = data[norm_col].fillna(0)
            
            return data
            
        except ResponseError as e:
            last_error = e
            error_msg = str(e)
            
            if '429' in error_msg:
                print(f"  â›” é‡åˆ°429é”™è¯¯ï¼ˆè¯·æ±‚è¿‡äºé¢‘ç¹ï¼‰")
                if attempt < MAX_RETRIES - 1:
                    countdown_sleep(RATE_LIMIT_SLEEP, "ç­‰å¾…APIé™åˆ¶è§£é™¤")
                    print(f"  ğŸ”„ é‡è¯•ä¸­... (å°è¯• {attempt + 2}/{MAX_RETRIES})")
            else:
                print(f"  âŒ APIé”™è¯¯: {error_msg}")
                if attempt < MAX_RETRIES - 1:
                    countdown_sleep(NORMAL_RETRY_DELAY, "ç­‰å¾…é‡è¯•")
                    print(f"  ğŸ”„ é‡è¯•ä¸­... (å°è¯• {attempt + 2}/{MAX_RETRIES})")
                    
        except Exception as e:
            last_error = e
            print(f"  âŒ æœªçŸ¥é”™è¯¯: {e}")
            if attempt < MAX_RETRIES - 1:
                countdown_sleep(NORMAL_RETRY_DELAY, "ç­‰å¾…é‡è¯•")
                print(f"  ğŸ”„ é‡è¯•ä¸­... (å°è¯• {attempt + 2}/{MAX_RETRIES})")
    
    print(f"  âœ— æ‰€æœ‰é‡è¯•å‡å¤±è´¥: {last_error}")
    return None


def save_batch_data(data, celebrity_names, season, output_dir):
    """
    ä»æ‰¹é‡æ•°æ®ä¸­æå–å¹¶ä¿å­˜æ¯ä¸ªæ˜æ˜Ÿçš„æ•°æ®
    
    Args:
        data: åŒ…å«æ‰€æœ‰æ˜æ˜Ÿæ•°æ®çš„DataFrame
        celebrity_names: æ˜æ˜Ÿå§“ååˆ—è¡¨
        season: èµ›å­£å·
        output_dir: è¾“å‡ºç›®å½•
    
    Returns:
        saved_names: æˆåŠŸä¿å­˜çš„æ˜æ˜Ÿåå­—åˆ—è¡¨
    """
    saved_names = []
    
    for celeb_name in celebrity_names:
        if celeb_name not in data.columns:
            print(f"  âš  '{celeb_name}' åœ¨è¿”å›æ•°æ®ä¸­ä¸å­˜åœ¨ï¼Œè·³è¿‡")
            continue
        
        # æå–è¯¥æ˜æ˜Ÿçš„æ•°æ®
        norm_col = f'{celeb_name}_normalized'
        cols_to_save = [celeb_name, ANCHOR_KEYWORD]
        if norm_col in data.columns:
            cols_to_save.append(norm_col)
        
        celeb_data = data[cols_to_save].copy()
        # é‡å‘½åå½’ä¸€åŒ–åˆ—ä¸ºç»Ÿä¸€çš„ 'normalized_score'
        if norm_col in celeb_data.columns:
            celeb_data = celeb_data.rename(columns={norm_col: 'normalized_score'})
        
        # åˆ›å»ºèµ›å­£å­ç›®å½•
        season_dir = os.path.join(output_dir, f'season_{season:02d}')
        os.makedirs(season_dir, exist_ok=True)
        
        # ä¿å­˜æ–‡ä»¶
        filepath = get_celebrity_filepath(celeb_name, season, output_dir)
        celeb_data.to_csv(filepath)
        saved_names.append(celeb_name)
        
        # æ‰“å°ç»Ÿè®¡æ‘˜è¦
        mean_raw = celeb_data[celeb_name].mean()
        max_raw = celeb_data[celeb_name].max()
        if 'normalized_score' in celeb_data.columns:
            mean_norm = celeb_data['normalized_score'].mean()
            max_norm = celeb_data['normalized_score'].max()
            print(f"    ğŸ“ˆ {celeb_name}: åŸå§‹(å‡å€¼={mean_raw:.1f}, æœ€å¤§={max_raw}) | å½’ä¸€åŒ–(å‡å€¼={mean_norm:.2f}, æœ€å¤§={max_norm:.2f})")
        else:
            print(f"    ğŸ“ˆ {celeb_name}: åŸå§‹(å‡å€¼={mean_raw:.1f}, æœ€å¤§={max_raw})")
    
    return saved_names


def sanitize_filename(name):
    """å°†åç§°è½¬æ¢ä¸ºå®‰å…¨çš„æ–‡ä»¶å"""
    invalid_chars = '<>:"/\\|?*'
    for char in invalid_chars:
        name = name.replace(char, '_')
    return name.strip(' .')


def get_celebrity_filepath(celebrity_name, season, output_dir):
    """è·å–æ˜æ˜Ÿæ•°æ®æ–‡ä»¶çš„å®Œæ•´è·¯å¾„"""
    season_dir = os.path.join(output_dir, f'season_{season:02d}')
    safe_name = sanitize_filename(celebrity_name)
    filename = f'{safe_name}.csv'
    return os.path.join(season_dir, filename)


def check_file_exists_and_valid(filepath, min_rows=2):
    """
    æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨ä¸”å†…å®¹æœ‰æ•ˆ
    
    Args:
        filepath: æ–‡ä»¶è·¯å¾„
        min_rows: æœ€å°‘è¡Œæ•°ï¼ˆåŒ…æ‹¬è¡¨å¤´ï¼‰ï¼Œé»˜è®¤2è¡¨ç¤ºè‡³å°‘æœ‰1è¡Œæ•°æ®
    
    Returns:
        bool: æ–‡ä»¶å­˜åœ¨ä¸”æœ‰æ•ˆè¿”å›Trueï¼Œå¦åˆ™è¿”å›False
    """
    if not os.path.exists(filepath):
        return False
    
    try:
        # æ£€æŸ¥æ–‡ä»¶å¤§å°ï¼ˆç©ºæ–‡ä»¶æˆ–åªæœ‰è¡¨å¤´çš„æ–‡ä»¶é€šå¸¸å¾ˆå°ï¼‰
        file_size = os.path.getsize(filepath)
        if file_size < 50:  # å°äº50å­—èŠ‚è®¤ä¸ºæ— æ•ˆ
            return False
        
        # å°è¯•è¯»å–å¹¶æ£€æŸ¥è¡Œæ•°
        df = pd.read_csv(filepath)
        if len(df) < (min_rows - 1):  # -1 å› ä¸ºè¡¨å¤´ä¸ç®—æ•°æ®è¡Œ
            return False
        
        return True
    except Exception:
        return False


def save_celebrity_data(data, celebrity_name, season, output_dir):
    """ä¿å­˜æ˜æ˜Ÿçš„trendsæ•°æ®åˆ°CSVæ–‡ä»¶"""
    # åˆ›å»ºèµ›å­£å­ç›®å½•
    season_dir = os.path.join(output_dir, f'season_{season:02d}')
    os.makedirs(season_dir, exist_ok=True)
    
    # è·å–æ–‡ä»¶è·¯å¾„
    filepath = get_celebrity_filepath(celebrity_name, season, output_dir)
    
    # ä¿å­˜åˆ°CSV
    data.to_csv(filepath)
    print(f"  ğŸ’¾ å·²ä¿å­˜: {filepath}")


def load_progress(output_dir):
    """åŠ è½½è¿›åº¦æ–‡ä»¶ä»¥æ”¯æŒæ–­ç‚¹ç»­ä¼ """
    progress_file = os.path.join(output_dir, 'progress.json')
    if os.path.exists(progress_file):
        with open(progress_file, 'r', encoding='utf-8') as f:
            return json.load(f)
    return {'completed': [], 'failed': []}


def save_progress(output_dir, progress):
    """ä¿å­˜è¿›åº¦æ–‡ä»¶"""
    progress_file = os.path.join(output_dir, 'progress.json')
    with open(progress_file, 'w', encoding='utf-8') as f:
        json.dump(progress, f, indent=2, ensure_ascii=False)


def get_celebrity_key(celebrity_name, season):
    """ä¸ºæ˜æ˜Ÿ-èµ›å­£ç»„åˆåˆ›å»ºå”¯ä¸€é”®"""
    return f"{celebrity_name}|{season}"


def random_delay():
    """éšæœºå»¶è¿Ÿä»¥é¿å…è¢«å°é”"""
    delay = random.uniform(MIN_DELAY, MAX_DELAY)
    countdown_sleep(delay, "è¯·æ±‚é—´éš”")


def main():
    """ä¸»å‡½æ•°ï¼šåè°ƒæ•°æ®æ”¶é›†æµç¨‹ï¼ˆæ‰¹é‡æ¨¡å¼ï¼‰"""
    print("=" * 65)
    print("  Dancing With The Stars - Google Trends æ•°æ®æ”¶é›†å™¨")
    print("  ä½¿ç”¨é”šç‚¹å½’ä¸€åŒ–æ–¹æ³• + æ‰¹é‡çˆ¬å–æ¨¡å¼ï¼ˆæ¯æ‰¹æœ€å¤š4ä¸ªæ˜æ˜Ÿï¼‰")
    print("=" * 65)
    print()
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    os.makedirs(OUTPUT_DIR, exist_ok=True)
    
    # åŠ è½½æ•°æ®
    season_times = load_season_times()
    celebrities = load_celebrities()
    
    # åŠ è½½è¿›åº¦
    progress = load_progress(OUTPUT_DIR)
    completed_set = set(progress['completed'])
    
    print(f"\nğŸ“Š è¿›åº¦ç»Ÿè®¡:")
    print(f"   å·²å®Œæˆ: {len(completed_set)} ä½æ˜æ˜Ÿ")
    print(f"   ä¹‹å‰å¤±è´¥: {len(progress['failed'])} ä½æ˜æ˜Ÿ")
    print(f"   é”šç‚¹å…³é”®è¯: \"{ANCHOR_KEYWORD}\"")
    print(f"   æ‰¹é‡å¤§å°: {BATCH_SIZE} æ˜æ˜Ÿ/æ‰¹")
    print()
    
    # æŒ‰èµ›å­£åˆ†ç»„æ˜æ˜Ÿ
    celebrities_by_season = {}
    for _, row in celebrities.iterrows():
        season = row['season']
        if season not in celebrities_by_season:
            celebrities_by_season[season] = []
        celebrities_by_season[season].append(row['celebrity_name'])
    
    # ç»Ÿè®¡
    total_celebrities = len(celebrities)
    success_count = 0
    fail_count = 0
    skip_count = 0
    batch_count = 0  # å®é™…çˆ¬å–çš„æ‰¹æ¬¡æ•°ï¼ˆç”¨äºå’–å•¡ä¼‘æ¯è®¡æ•°ï¼‰
    
    # æŒ‰èµ›å­£å¤„ç†
    sorted_seasons = sorted(celebrities_by_season.keys())
    
    for season in sorted_seasons:
        celeb_list = celebrities_by_season[season]
        
        # æ£€æŸ¥èµ›å­£æ˜¯å¦å­˜åœ¨
        if season not in season_times:
            print(f"\nâš  ç¬¬ {season} å­£ä¸åœ¨ season-time.csv ä¸­ï¼Œè·³è¿‡è¯¥èµ›å­£æ‰€æœ‰æ˜æ˜Ÿ")
            for celeb_name in celeb_list:
                progress['failed'].append({
                    'name': celeb_name, 
                    'season': season, 
                    'reason': 'èµ›å­£æœªæ‰¾åˆ°'
                })
                fail_count += 1
            continue
        
        start_date, end_date = season_times[season]
        
        # è¿‡æ»¤å‡ºæœªå®Œæˆçš„æ˜æ˜Ÿ
        pending_celebs = []
        for celeb_name in celeb_list:
            key = get_celebrity_key(celeb_name, season)
            filepath = get_celebrity_filepath(celeb_name, season, OUTPUT_DIR)
            
            if key in completed_set:
                skip_count += 1
                continue
            
            if check_file_exists_and_valid(filepath):
                # æ–‡ä»¶å­˜åœ¨ä½†ä¸åœ¨completed_setä¸­ï¼ŒåŒæ­¥æ›´æ–°
                progress['completed'].append(key)
                completed_set.add(key)
                save_progress(OUTPUT_DIR, progress)
                skip_count += 1
                continue
            
            pending_celebs.append(celeb_name)
        
        if not pending_celebs:
            print(f"\nç¬¬ {season} å­£: æ‰€æœ‰ {len(celeb_list)} ä½æ˜æ˜Ÿå·²å®Œæˆï¼Œè·³è¿‡")
            continue
        
        print(f"\n{'='*50}")
        print(f"ç¬¬ {season} å­£ ({start_date} è‡³ {end_date})")
        print(f"å¾…å¤„ç†: {len(pending_celebs)}/{len(celeb_list)} ä½æ˜æ˜Ÿ")
        print(f"{'='*50}")
        
        # å°†å¾…å¤„ç†æ˜æ˜Ÿåˆ†æ‰¹ï¼ˆæ¯æ‰¹æœ€å¤šBATCH_SIZEä¸ªï¼‰
        batches = [pending_celebs[i:i+BATCH_SIZE] for i in range(0, len(pending_celebs), BATCH_SIZE)]
        
        for batch_idx, batch in enumerate(batches):
            batch_count += 1
            
            print(f"\n  ğŸ“¦ æ‰¹æ¬¡ {batch_idx + 1}/{len(batches)}: {batch}")
            
            # è·å–æ•°æ®
            data = get_google_trends_with_anchor(batch, start_date, end_date)
            
            if data is not None:
                # ä¿å­˜æ¯ä¸ªæ˜æ˜Ÿçš„æ•°æ®
                saved_names = save_batch_data(data, batch, season, OUTPUT_DIR)
                
                for celeb_name in batch:
                    key = get_celebrity_key(celeb_name, season)
                    if celeb_name in saved_names:
                        progress['completed'].append(key)
                        completed_set.add(key)
                        success_count += 1
                        print(f"    âœ“ {celeb_name} ä¿å­˜æˆåŠŸ")
                    else:
                        progress['failed'].append({
                            'name': celeb_name, 
                            'season': season, 
                            'reason': 'æ•°æ®æå–å¤±è´¥'
                        })
                        fail_count += 1
                        print(f"    âœ— {celeb_name} ä¿å­˜å¤±è´¥")
            else:
                # æ•´æ‰¹å¤±è´¥
                for celeb_name in batch:
                    key = get_celebrity_key(celeb_name, season)
                    progress['failed'].append({
                        'name': celeb_name, 
                        'season': season, 
                        'reason': 'æ‰¹æ¬¡è¯·æ±‚å¤±è´¥'
                    })
                    fail_count += 1
                print(f"    âœ— æ•´æ‰¹è¯·æ±‚å¤±è´¥")
            
            # ä¿å­˜è¿›åº¦
            save_progress(OUTPUT_DIR, progress)
            
            # æ£€æŸ¥æ˜¯å¦éœ€è¦å’–å•¡ä¼‘æ¯ï¼ˆæ¯å¤„ç†ä¸€å®šæ•°é‡çš„æ‰¹æ¬¡ï¼‰
            if batch_count > 0 and batch_count % COFFEE_BREAK_INTERVAL == 0:
                print(f"\n  â˜• å’–å•¡ä¼‘æ¯æ—¶é—´ï¼(å·²å¤„ç† {batch_count} æ‰¹)")
                countdown_sleep(COFFEE_BREAK_DURATION, "å’–å•¡ä¼‘æ¯")
                print(f"  âœ“ ä¼‘æ¯å®Œæ¯•ï¼Œç»§ç»­å·¥ä½œ...")
            
            # æ¯æ‰¹çˆ¬å–åéšæœºå»¶è¿Ÿï¼ˆé˜²æ­¢è¢«å°ï¼‰
            random_delay()
    
    # æœ€ç»ˆæ€»ç»“
    print("\n" + "=" * 65)
    print("  æ•°æ®æ”¶é›†å®Œæˆï¼")
    print("=" * 65)
    print(f"  æ€»æ˜æ˜Ÿæ•°: {total_celebrities}")
    print(f"  âœ“ æˆåŠŸè·å–: {success_count}")
    print(f"  â­ è·³è¿‡ï¼ˆå·²å®Œæˆï¼‰: {skip_count}")
    print(f"  âœ— å¤±è´¥: {fail_count}")
    print(f"  ğŸ“¦ æ€»æ‰¹æ¬¡æ•°: {batch_count}")
    print(f"\n  æ•°æ®ä¿å­˜ä½ç½®: {OUTPUT_DIR}")
    
    # ä¿å­˜æœ€ç»ˆè¿›åº¦
    save_progress(OUTPUT_DIR, progress)
    
    # åˆ›å»ºæ±‡æ€»æ–‡ä»¶
    create_summary(OUTPUT_DIR, season_times, celebrities, progress)


def create_summary(output_dir, season_times, celebrities, progress):
    """åˆ›å»ºåŒ…å«æ‰€æœ‰æ”¶é›†æ•°æ®ä¿¡æ¯çš„æ±‡æ€»CSV"""
    summary_file = os.path.join(output_dir, 'collection_summary.csv')
    
    summary_data = []
    for _, row in celebrities.iterrows():
        celebrity_name = row['celebrity_name']
        season = row['season']
        key = get_celebrity_key(celebrity_name, season)
        
        status = 'completed' if key in progress['completed'] else 'failed'
        start_date, end_date = season_times.get(season, ('N/A', 'N/A'))
        
        summary_data.append({
            'celebrity_name': celebrity_name,
            'season': season,
            'start_date': start_date,
            'end_date': end_date,
            'status': status
        })
    
    summary_df = pd.DataFrame(summary_data)
    summary_df.to_csv(summary_file, index=False)
    print(f"\n  ğŸ“‹ æ±‡æ€»æ–‡ä»¶å·²ä¿å­˜: {summary_file}")


if __name__ == '__main__':
    main()
